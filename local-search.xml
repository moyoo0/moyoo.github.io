<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>softmax回归</title>
    <link href="/2024/10/26/softmax%E5%9B%9E%E5%BD%92/"/>
    <url>/2024/10/26/softmax%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="1-softmax回归原理"><a href="#1-softmax回归原理" class="headerlink" title="1. softmax回归原理"></a>1. softmax回归原理</h2><h3 id="1-1-分类和回归"><a href="#1-1-分类和回归" class="headerlink" title="1.1 分类和回归"></a>1.1 分类和回归</h3><p><strong>分类</strong>：分类任务的目标是预测一个<strong>离散的类别标签</strong>。例如，将图片分为猫和狗，预测邮件是垃圾邮件还是正常邮件等。类别标签通常是有限的，常见的标签形式为整数（如 0, 1, 2）或具体的分类名称（如“猫”，“狗”）。</p><p><strong>回归</strong>：回归任务的目标是预测一个<strong>连续的数值</strong>。例如，预测房价、股票价格、温度等。在回归问题中，输出值可以是任意实数。</p><h3 id="1-2-网络架构"><a href="#1-2-网络架构" class="headerlink" title="1.2 网络架构"></a>1.2 网络架构</h3><p>为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。</p><p>在上述例子（猫，鸡，狗）中，我们假设对于一个样本有4个特征，因此对于原始的样本其维度为[n, 4]，我们要将其最终映射到[n, 3]，其中“3”表示one-hot编码，例如我们可以假定<br>(1,0,0)对应于“狗”，(0,1,0)对应于“鸡”，(0,0,1)对应于“狗”。所以我们需要$w$的维度为[3, 4], $b$的维度为[3, 1]，如下图所示，$o1,o2,o3$表示类别</p><p><img src="/2024/10/26/softmax%E5%9B%9E%E5%BD%92/softmax.jpg"></p><h3 id="1-3-softmax函数"><a href="#1-3-softmax函数" class="headerlink" title="1.3 softmax函数"></a>1.3 softmax函数</h3><p>Softmax 是一种常用的函数，用于多分类问题中将模型的输出值（logits）转换为概率分布。它能够将任意实数的输出值映射到一个区间为 <code>[0, 1]</code> 的概率空间，并且保证这些概率之和为 <code>1</code>。下面是对 Softmax 的详细介绍：</p><p>对于给定的输入向量 $z &#x3D; [z_1, z_2, \dots, z_n]$，Softmax 函数的输出为一个概率分布向量 $\sigma(z) &#x3D; [\sigma(z_1), \sigma(z_2), \dots, \sigma(z_n)]$，其中每个元素的计算公式如下：<br>$$<br>\sigma(z_i) &#x3D; \frac{e^{z_i}}{\sum_{j&#x3D;1}^{n} e^{z_j}}<br>$$</p><ul><li><strong>分子</strong>：将每个元素 $z_i$映射为非负值 $e^{z_i}$。</li><li><strong>分母</strong>：是所有输入元素的指数和，确保输出的所有元素之和为 1。</li></ul><h3 id="1-4-交叉熵损失"><a href="#1-4-交叉熵损失" class="headerlink" title="1.4 交叉熵损失"></a>1.4 交叉熵损失</h3><p>交叉熵损失函数可化简为$\text{CrossEntropy} &#x3D; - \sum_{i&#x3D;1}^{N} \log(\hat{y}<em>{i, y_i})$,因为当且仅当y为真是才等于1，否则等于零，其完整公式为：$\text{CrossEntropy} &#x3D; - \sum</em>{i&#x3D;1}^{N} \sum_{j&#x3D;1}^{C} y_{i,j} \cdot \log(\hat{y}_{i,j})$</p><h2 id="2-一个例子"><a href="#2-一个例子" class="headerlink" title="2. 一个例子"></a>2. 一个例子</h2><h3 id="2-1-数据集"><a href="#2-1-数据集" class="headerlink" title="2.1 数据集"></a>2.1 数据集</h3><p>数据集来源于Fashion-mnist，主要通过softmax回归实现对若干图片的分类，例如：</p><p><img src="/2024/10/26/softmax%E5%9B%9E%E5%BD%92/minist.jpg" alt="部分数据展示"></p><h3 id="2-2-代码"><a href="#2-2-代码" class="headerlink" title="2.2 代码"></a>2.2 代码</h3><h4 id="2-2-0-读取数据集"><a href="#2-2-0-读取数据集" class="headerlink" title="2.2.0 读取数据集"></a>2.2.0 读取数据集</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br></code></pre></td></tr></table></figure><p>将数据集读取到两个迭代器中，并把mini-batch的值设为256</p><h4 id="2-2-1-输出层定义"><a href="#2-2-1-输出层定义" class="headerlink" title="2.2.1 输出层定义"></a>2.2.1 输出层定义</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># PytTorch不会隐式地调整输入的形状。因此，我们定义了展平层（flatten）在线性层前调整网络输入的形状</span><br>net = nn.Sequential(nn.Flatten(), nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">10</span>))<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>    <span class="hljs-comment"># m为当前层</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear:<br>        <span class="hljs-comment"># 使用均值为0、标准差为0.01的正态分布随机初始化权重</span><br>        nn.init.normal_(m.weight, std=<span class="hljs-number">0.01</span>)<br><br>net.apply(init_weights)<br></code></pre></td></tr></table></figure><ul><li><p>每个图片规格为28*28，因此先展平成784的向量。</p></li><li><p><code>nn.Linear(784, 10)</code>：这是一个全连接层（线性层），输入大小为 784，输出大小为 10，常用于将拉平后的数据映射到一个特定的输出空间（如 10 类分类任务）。</p></li><li><p><code>if type(m) == nn.Linear</code>：检查 <code>m</code> 是否为线性层。只有在该层为线性层的情况下，才进行权重初始化。</p></li><li><p><code>net.apply(init_weights)</code> 会遍历 <code>net</code> 中的所有层，并将每一层传入 <code>init_weights</code> 函数。这样只有线性层的权重会被初始化为指定的正态分布，而其他层（如 <code>nn.Flatten</code>）则保持不变。</p></li></ul><h4 id="2-2-2-交叉熵损失"><a href="#2-2-2-交叉熵损失" class="headerlink" title="2.2.2 交叉熵损失"></a>2.2.2 交叉熵损失</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 交叉熵损失函数, nn.CrossEntropyLoss默认是对预测值做softmax,</span><br><span class="hljs-comment"># 所以我们的网络输出层不应用softmax</span><br><span class="hljs-comment"># 因为交叉熵损失函数的计算公式中已经包含了softmax运算</span><br>loss = nn.CrossEntropyLoss()<br></code></pre></td></tr></table></figure><h4 id="2-2-3-优化算法"><a href="#2-2-3-优化算法" class="headerlink" title="2.2.3 优化算法"></a>2.2.3 优化算法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 优化算法, 使用小批量随机梯度下降, 学习率为0.1</span><br>trainer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.1</span>)<br></code></pre></td></tr></table></figure><h4 id="2-2-4-训练"><a href="#2-2-4-训练" class="headerlink" title="2.2.4 训练"></a>2.2.4 训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">num_epochs = <span class="hljs-number">10</span><br>train(net, train_iter, test_iter, loss, num_epochs, trainer)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch中的backward（）操作相关内容</title>
    <link href="/2024/10/26/pytorch%E4%B8%AD%E7%9A%84backward%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/"/>
    <url>/2024/10/26/pytorch%E4%B8%AD%E7%9A%84backward%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="1-PyTorch-利用-backward-求梯度的过程和原理"><a href="#1-PyTorch-利用-backward-求梯度的过程和原理" class="headerlink" title="1. PyTorch 利用 backward() 求梯度的过程和原理"></a>1. PyTorch 利用 <code>backward()</code> 求梯度的过程和原理</h2><p>在 PyTorch 中，<code>backward()</code> 是自动求导的关键函数，它用于计算张量的梯度。PyTorch 的自动求导功能基于“计算图”和“反向传播”原理，使得在训练深度学习模型时，计算损失函数相对于各参数的梯度变得高效便捷。下面介绍 PyTorch 中 <code>backward()</code> 求梯度的过程和原理。</p><h3 id="1-1-计算图（Computation-Graph）"><a href="#1-1-计算图（Computation-Graph）" class="headerlink" title="1.1 计算图（Computation Graph）"></a>1.1 计算图（Computation Graph）</h3><p>在 PyTorch 中，每个操作（如加法、乘法）都会创建一个计算节点，将这些节点连接起来形成<strong>计算图</strong>。这个图是有向无环图 (Directed Acyclic Graph, DAG)，从输入数据开始，一直到最终的输出。在计算图中：</p><ul><li>每个节点表示一个张量操作。</li><li>每条边表示操作之间的依赖关系。</li></ul><p>通过构建计算图，PyTorch 可以追踪到所有操作以及操作之间的依赖关系，为后续的反向传播提供了依据。</p><h3 id="1-2-反向传播（Backpropagation）"><a href="#1-2-反向传播（Backpropagation）" class="headerlink" title="1.2 反向传播（Backpropagation）"></a>1.2 反向传播（Backpropagation）</h3><p>反向传播是一种计算梯度的算法，利用链式法则，从输出层开始，反向逐层传播梯度。PyTorch 的 <code>backward()</code> 函数使用反向传播算法来自动计算梯度，具体过程如下：</p><ol><li><strong>前向传播</strong>：将输入数据经过网络，进行一系列操作得到最终输出和损失。</li><li><strong>计算损失的梯度</strong>：<ul><li>在调用 <code>backward()</code> 时，PyTorch 会从损失函数开始，沿计算图反向传播。</li><li>对于每一个张量的梯度 <code>∂L/∂x</code>（<code>L</code> 是损失函数，<code>x</code> 是张量），通过链式法则（即将每一步的梯度相乘）逐步计算梯度。</li></ul></li><li><strong>计算每层权重的梯度</strong>：将梯度传播至所有的叶子节点，即包含 <code>requires_grad=True</code> 的参数。此时，每个张量 <code>x</code> 的 <code>x.grad</code> 中会保存损失函数 <code>L</code> 对该张量的偏导数。</li></ol><h3 id="1-3-使用-backward-的步骤"><a href="#1-3-使用-backward-的步骤" class="headerlink" title="1.3 使用 backward() 的步骤"></a>1.3 使用 <code>backward()</code> 的步骤</h3><p>以下是 PyTorch 中使用 <code>backward()</code> 计算梯度的典型流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 创建张量并启用梯度计算</span><br>x = torch.tensor([<span class="hljs-number">2.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>y = x ** <span class="hljs-number">2</span><br>z = y * <span class="hljs-number">3</span><br><br><span class="hljs-comment"># 假设损失是z，计算梯度</span><br>z.backward()<br><br><span class="hljs-comment"># x.grad 包含 dz/dx 的值</span><br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor([12.])</span><br></code></pre></td></tr></table></figure><h3 id="1-4-pytorch中梯度的累加"><a href="#1-4-pytorch中梯度的累加" class="headerlink" title="1.4 pytorch中梯度的累加"></a>1.4 pytorch中梯度的累加</h3><p>在 PyTorch 中，每个张量都有一个属性 <code>grad</code>，用来存储梯度值。当我们调用 <code>backward()</code> 时，计算的梯度值会被累加到该属性中。这意味着：</p><ul><li>多次调用 <code>backward()</code>，梯度会不断累加；</li><li>累加后的梯度值在优化步骤中被用于更新模型参数。</li></ul><p>通常情况下，如果每次梯度更新前不进行清零操作<code>optimizer.zero_grad()</code>，前一步计算的梯度将残留在 <code>grad</code> 属性中，影响下一步的梯度计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 假设 model 是定义好的神经网络模型</span><br>optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> dataloader:<br>    optimizer.zero_grad()       <span class="hljs-comment"># 清零梯度</span><br>    output = model(data)        <span class="hljs-comment"># 前向传播</span><br>    loss = loss_fn(output, target)  <span class="hljs-comment"># 计算损失</span><br>    loss.backward()             <span class="hljs-comment"># 反向传播，计算梯度</span><br>    optimizer.step()            <span class="hljs-comment"># 更新模型参数</span><br><br></code></pre></td></tr></table></figure><p>在这里，<code>optimizer.zero_grad()</code> 确保每次反向传播前梯度清零，以免累加前一批次的数据产生的梯度。</p><h2 id="2-DataLoader"><a href="#2-DataLoader" class="headerlink" title="2.DataLoader"></a>2.DataLoader</h2><p>在 PyTorch 中，<code>DataLoader</code> 是用于加载数据的工具，可以帮助我们将数据按批次（batch）加载，并进行必要的预处理，如打乱数据、并行加载等。<code>DataLoader</code> 尤其适合在深度学习模型训练过程中高效地处理和管理数据。</p><h3 id="2-1-DataLoader-的作用"><a href="#2-1-DataLoader-的作用" class="headerlink" title="2.1 DataLoader 的作用"></a>2.1 DataLoader 的作用</h3><p><code>DataLoader</code> 的核心功能包括：</p><ul><li><strong>按批次加载数据</strong>：可以将数据集分为多个小批量（batch），使得每次训练迭代只用一个小批量数据，这对内存使用和优化效果有很大帮助。</li><li><strong>打乱数据</strong>：在每次 epoch 之前随机打乱数据的顺序，以提高模型的泛化能力。</li><li><strong>并行处理</strong>：通过多线程或多进程的方式加速数据加载，减少数据读取的时间消耗。</li></ul><h3 id="2-2-DataLoader-的基本用法"><a href="#2-2-DataLoader-的基本用法" class="headerlink" title="2.2 DataLoader 的基本用法"></a>2.2 DataLoader 的基本用法</h3><p>使用 <code>DataLoader</code> 的步骤主要有两个：</p><ol><li><strong>定义数据集</strong>：可以使用 PyTorch 内置的数据集（如 <code>torchvision.datasets</code>）或自定义数据集。</li><li><strong>创建 DataLoader 对象</strong>：将数据集传入 <code>DataLoader</code>，并设置批次大小和其他参数。</li></ol><p>以下是一个简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, TensorDataset<br><br><span class="hljs-comment"># 创建示例数据</span><br>data = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>).<span class="hljs-built_in">float</span>().view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 10个样本，每个样本1维</span><br>targets = data * <span class="hljs-number">2</span>  <span class="hljs-comment"># 假设目标是输入的2倍</span><br><br><span class="hljs-comment"># 创建TensorDataset</span><br>dataset = TensorDataset(data, targets)<br><br><span class="hljs-comment"># 使用DataLoader加载数据</span><br>dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 迭代DataLoader</span><br><span class="hljs-keyword">for</span> batch_data, batch_targets <span class="hljs-keyword">in</span> dataloader:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Data:&quot;</span>, batch_data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Targets:&quot;</span>, batch_targets)<br><br></code></pre></td></tr></table></figure><h3 id="2-3-DataLoader-的参数说明"><a href="#2-3-DataLoader-的参数说明" class="headerlink" title="2.3 DataLoader 的参数说明"></a>2.3 DataLoader 的参数说明</h3><p><code>DataLoader</code> 的常用参数包括：</p><ul><li><strong>dataset</strong>：要加载的数据集对象。</li><li><strong>batch_size</strong>：每个批次的样本数量。</li><li><strong>shuffle</strong>：是否在每次迭代时打乱数据（一般在训练集上启用）。</li><li><strong>num_workers</strong>：用于加载数据的工作线程数量，通常在 GPU 训练时可以增加这个值以提高数据加载速度。</li><li><strong>drop_last</strong>：若 <code>True</code>，在数据大小不能整除 <code>batch_size</code> 时，丢弃最后一个不足批次的数据；若 <code>False</code> 则保留。</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图片测试</title>
    <link href="/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/"/>
    <url>/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<p><img src="/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/test2.jpg" alt="图片测试"></p>]]></content>
    
    
    
    <tags>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于上传博客</title>
    <link href="/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h2 id="上传博客命令"><a href="#上传博客命令" class="headerlink" title="上传博客命令"></a>上传博客命令</h2><h3 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo new “文章名称”<br></code></pre></td></tr></table></figure><h3 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo g <span class="hljs-literal">-d</span><br>npx hexo s<br></code></pre></td></tr></table></figure><h3 id="确认无误后先生成一编文件"><a href="#确认无误后先生成一编文件" class="headerlink" title="确认无误后先生成一编文件"></a>确认无误后先生成一编文件</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo g<br></code></pre></td></tr></table></figure><h3 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo d<br></code></pre></td></tr></table></figure><h3 id="有关图片上传"><a href="#有关图片上传" class="headerlink" title="有关图片上传"></a>有关图片上传</h3><p>使用的是name.jpg，图片要放到同名目录下，并且本地编译后图片需要赋值到public中的相应目录，然后再提交，有时需多刷新几次。</p><p><img src="/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/test1.jpg" alt="图片引用方法"></p>]]></content>
    
    
    
    <tags>
      
      <tag>博客上传</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p><img src="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.jpg" alt="图片引用方法二"></p><p><img src="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test1.jpg" alt="图片引用方法三"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/10/26/hello-world/"/>
    <url>/2024/10/26/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="reate-a-new-post"><a href="#reate-a-new-post" class="headerlink" title="reate a new post"></a>reate a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
