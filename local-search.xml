<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>pytorch中的backward（）操作相关内容</title>
    <link href="/2024/10/26/pytorch%E4%B8%AD%E7%9A%84backward%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/"/>
    <url>/2024/10/26/pytorch%E4%B8%AD%E7%9A%84backward%E6%93%8D%E4%BD%9C%E7%9B%B8%E5%85%B3%E5%86%85%E5%AE%B9/</url>
    
    <content type="html"><![CDATA[<h2 id="1-PyTorch-利用-backward-求梯度的过程和原理"><a href="#1-PyTorch-利用-backward-求梯度的过程和原理" class="headerlink" title="1. PyTorch 利用 backward() 求梯度的过程和原理"></a>1. PyTorch 利用 <code>backward()</code> 求梯度的过程和原理</h2><p>在 PyTorch 中，<code>backward()</code> 是自动求导的关键函数，它用于计算张量的梯度。PyTorch 的自动求导功能基于“计算图”和“反向传播”原理，使得在训练深度学习模型时，计算损失函数相对于各参数的梯度变得高效便捷。下面介绍 PyTorch 中 <code>backward()</code> 求梯度的过程和原理。</p><h3 id="1-1-计算图（Computation-Graph）"><a href="#1-1-计算图（Computation-Graph）" class="headerlink" title="1.1 计算图（Computation Graph）"></a>1.1 计算图（Computation Graph）</h3><p>在 PyTorch 中，每个操作（如加法、乘法）都会创建一个计算节点，将这些节点连接起来形成<strong>计算图</strong>。这个图是有向无环图 (Directed Acyclic Graph, DAG)，从输入数据开始，一直到最终的输出。在计算图中：</p><ul><li>每个节点表示一个张量操作。</li><li>每条边表示操作之间的依赖关系。</li></ul><p>通过构建计算图，PyTorch 可以追踪到所有操作以及操作之间的依赖关系，为后续的反向传播提供了依据。</p><h3 id="1-2-反向传播（Backpropagation）"><a href="#1-2-反向传播（Backpropagation）" class="headerlink" title="1.2 反向传播（Backpropagation）"></a>1.2 反向传播（Backpropagation）</h3><p>反向传播是一种计算梯度的算法，利用链式法则，从输出层开始，反向逐层传播梯度。PyTorch 的 <code>backward()</code> 函数使用反向传播算法来自动计算梯度，具体过程如下：</p><ol><li><strong>前向传播</strong>：将输入数据经过网络，进行一系列操作得到最终输出和损失。</li><li><strong>计算损失的梯度</strong>：<ul><li>在调用 <code>backward()</code> 时，PyTorch 会从损失函数开始，沿计算图反向传播。</li><li>对于每一个张量的梯度 <code>∂L/∂x</code>（<code>L</code> 是损失函数，<code>x</code> 是张量），通过链式法则（即将每一步的梯度相乘）逐步计算梯度。</li></ul></li><li><strong>计算每层权重的梯度</strong>：将梯度传播至所有的叶子节点，即包含 <code>requires_grad=True</code> 的参数。此时，每个张量 <code>x</code> 的 <code>x.grad</code> 中会保存损失函数 <code>L</code> 对该张量的偏导数。</li></ol><h3 id="1-3-使用-backward-的步骤"><a href="#1-3-使用-backward-的步骤" class="headerlink" title="1.3 使用 backward() 的步骤"></a>1.3 使用 <code>backward()</code> 的步骤</h3><p>以下是 PyTorch 中使用 <code>backward()</code> 计算梯度的典型流程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 创建张量并启用梯度计算</span><br>x = torch.tensor([<span class="hljs-number">2.0</span>], requires_grad=<span class="hljs-literal">True</span>)<br>y = x ** <span class="hljs-number">2</span><br>z = y * <span class="hljs-number">3</span><br><br><span class="hljs-comment"># 假设损失是z，计算梯度</span><br>z.backward()<br><br><span class="hljs-comment"># x.grad 包含 dz/dx 的值</span><br><span class="hljs-built_in">print</span>(x.grad)  <span class="hljs-comment"># 输出: tensor([12.])</span><br></code></pre></td></tr></table></figure><h3 id="1-4-pytorch中梯度的累加"><a href="#1-4-pytorch中梯度的累加" class="headerlink" title="1.4 pytorch中梯度的累加"></a>1.4 pytorch中梯度的累加</h3><p>在 PyTorch 中，每个张量都有一个属性 <code>grad</code>，用来存储梯度值。当我们调用 <code>backward()</code> 时，计算的梯度值会被累加到该属性中。这意味着：</p><ul><li>多次调用 <code>backward()</code>，梯度会不断累加；</li><li>累加后的梯度值在优化步骤中被用于更新模型参数。</li></ul><p>通常情况下，如果每次梯度更新前不进行清零操作<code>optimizer.zero_grad()</code>，前一步计算的梯度将残留在 <code>grad</code> 属性中，影响下一步的梯度计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 假设 model 是定义好的神经网络模型</span><br>optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">0.01</span>)<br><br><span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> dataloader:<br>    optimizer.zero_grad()       <span class="hljs-comment"># 清零梯度</span><br>    output = model(data)        <span class="hljs-comment"># 前向传播</span><br>    loss = loss_fn(output, target)  <span class="hljs-comment"># 计算损失</span><br>    loss.backward()             <span class="hljs-comment"># 反向传播，计算梯度</span><br>    optimizer.step()            <span class="hljs-comment"># 更新模型参数</span><br><br></code></pre></td></tr></table></figure><p>在这里，<code>optimizer.zero_grad()</code> 确保每次反向传播前梯度清零，以免累加前一批次的数据产生的梯度。</p><h2 id="2-DataLoader"><a href="#2-DataLoader" class="headerlink" title="2.DataLoader"></a>2.DataLoader</h2><p>在 PyTorch 中，<code>DataLoader</code> 是用于加载数据的工具，可以帮助我们将数据按批次（batch）加载，并进行必要的预处理，如打乱数据、并行加载等。<code>DataLoader</code> 尤其适合在深度学习模型训练过程中高效地处理和管理数据。</p><h3 id="2-1-DataLoader-的作用"><a href="#2-1-DataLoader-的作用" class="headerlink" title="2.1 DataLoader 的作用"></a>2.1 DataLoader 的作用</h3><p><code>DataLoader</code> 的核心功能包括：</p><ul><li><strong>按批次加载数据</strong>：可以将数据集分为多个小批量（batch），使得每次训练迭代只用一个小批量数据，这对内存使用和优化效果有很大帮助。</li><li><strong>打乱数据</strong>：在每次 epoch 之前随机打乱数据的顺序，以提高模型的泛化能力。</li><li><strong>并行处理</strong>：通过多线程或多进程的方式加速数据加载，减少数据读取的时间消耗。</li></ul><h3 id="2-2-DataLoader-的基本用法"><a href="#2-2-DataLoader-的基本用法" class="headerlink" title="2.2 DataLoader 的基本用法"></a>2.2 DataLoader 的基本用法</h3><p>使用 <code>DataLoader</code> 的步骤主要有两个：</p><ol><li><strong>定义数据集</strong>：可以使用 PyTorch 内置的数据集（如 <code>torchvision.datasets</code>）或自定义数据集。</li><li><strong>创建 DataLoader 对象</strong>：将数据集传入 <code>DataLoader</code>，并设置批次大小和其他参数。</li></ol><p>以下是一个简单的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, TensorDataset<br><br><span class="hljs-comment"># 创建示例数据</span><br>data = torch.arange(<span class="hljs-number">1</span>, <span class="hljs-number">11</span>).<span class="hljs-built_in">float</span>().view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 10个样本，每个样本1维</span><br>targets = data * <span class="hljs-number">2</span>  <span class="hljs-comment"># 假设目标是输入的2倍</span><br><br><span class="hljs-comment"># 创建TensorDataset</span><br>dataset = TensorDataset(data, targets)<br><br><span class="hljs-comment"># 使用DataLoader加载数据</span><br>dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 迭代DataLoader</span><br><span class="hljs-keyword">for</span> batch_data, batch_targets <span class="hljs-keyword">in</span> dataloader:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Data:&quot;</span>, batch_data)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Targets:&quot;</span>, batch_targets)<br><br></code></pre></td></tr></table></figure><h3 id="2-3-DataLoader-的参数说明"><a href="#2-3-DataLoader-的参数说明" class="headerlink" title="2.3 DataLoader 的参数说明"></a>2.3 DataLoader 的参数说明</h3><p><code>DataLoader</code> 的常用参数包括：</p><ul><li><strong>dataset</strong>：要加载的数据集对象。</li><li><strong>batch_size</strong>：每个批次的样本数量。</li><li><strong>shuffle</strong>：是否在每次迭代时打乱数据（一般在训练集上启用）。</li><li><strong>num_workers</strong>：用于加载数据的工作线程数量，通常在 GPU 训练时可以增加这个值以提高数据加载速度。</li><li><strong>drop_last</strong>：若 <code>True</code>，在数据大小不能整除 <code>batch_size</code> 时，丢弃最后一个不足批次的数据；若 <code>False</code> 则保留。</li></ul>]]></content>
    
    
    <categories>
      
      <category>深度学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图片测试</title>
    <link href="/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/"/>
    <url>/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<p><img src="/2024/10/26/%E5%9B%BE%E7%89%87%E6%B5%8B%E8%AF%95/test2.jpg" alt="图片测试"></p>]]></content>
    
    
    
    <tags>
      
      <tag>测试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于上传博客</title>
    <link href="/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/"/>
    <url>/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<h2 id="上传博客命令"><a href="#上传博客命令" class="headerlink" title="上传博客命令"></a>上传博客命令</h2><h3 id="新建文章"><a href="#新建文章" class="headerlink" title="新建文章"></a>新建文章</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo new “文章名称”<br></code></pre></td></tr></table></figure><h3 id="本地预览"><a href="#本地预览" class="headerlink" title="本地预览"></a>本地预览</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo g <span class="hljs-literal">-d</span><br>npx hexo s<br></code></pre></td></tr></table></figure><h3 id="确认无误后先生成一编文件"><a href="#确认无误后先生成一编文件" class="headerlink" title="确认无误后先生成一编文件"></a>确认无误后先生成一编文件</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo g<br></code></pre></td></tr></table></figure><h3 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h3><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">npx hexo d<br></code></pre></td></tr></table></figure><h3 id="有关图片上传"><a href="#有关图片上传" class="headerlink" title="有关图片上传"></a>有关图片上传</h3><p>使用的是name.jpg，图片要放到同名目录下，并且本地编译后图片需要赋值到public中的相应目录，然后再提交，有时需多刷新几次。</p><p><img src="/2024/10/26/%E5%85%B3%E4%BA%8E%E4%B8%8A%E4%BC%A0%E5%8D%9A%E5%AE%A2/test1.jpg" alt="图片引用方法"></p>]]></content>
    
    
    
    <tags>
      
      <tag>博客上传</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>测试文章</title>
    <link href="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <url>/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>这是一篇测试文章</p><p><img src="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test.jpg" alt="图片引用方法二"></p><p><img src="/2024/10/26/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/test1.jpg" alt="图片引用方法三"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/10/26/hello-world/"/>
    <url>/2024/10/26/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="reate-a-new-post"><a href="#reate-a-new-post" class="headerlink" title="reate a new post"></a>reate a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
